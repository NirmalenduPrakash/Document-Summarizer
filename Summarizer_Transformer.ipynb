{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarizer-Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP4gdcgN5b4tz0yr6hqROkP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NirmalenduPrakash/Document-Summarizer/blob/master/Summarizer_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im6e5-c_DSCa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e68816d0-cbec-41f1-ee2a-bc92b04bbf5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzv8hCY9DV2e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "9f93fd0d-c113-4b16-d3ea-fc500ec233cd"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Transformer-master')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b459f319f3ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Transformer-master'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Transformer-master'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Qj0Sx0Dm6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "9fffa14b-4f6f-4a4e-b80e-233dc172b98b"
      },
      "source": [
        "!python train.py -src_data data/english.txt -trg_data data/french.txt -src_lang en -trg_lang fr -no_cuda"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading spacy tokenizers...\n",
            "creating dataset and iterator... \n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "training model...\n",
            "torch.Size([166, 8, 7, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 7, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 7, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 7, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 7, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 7, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 8, 8]) torch.Size([166, 8, 8, 64])\n",
            "torch.Size([166, 8, 8, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 8, 8]) torch.Size([166, 8, 8, 64])\n",
            "torch.Size([166, 8, 8, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 8, 8]) torch.Size([166, 8, 8, 64])\n",
            "torch.Size([166, 8, 8, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 8, 8]) torch.Size([166, 8, 8, 64])\n",
            "torch.Size([166, 8, 8, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 8, 8]) torch.Size([166, 8, 8, 64])\n",
            "torch.Size([166, 8, 8, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([166, 8, 8, 8]) torch.Size([166, 8, 8, 64])\n",
            "torch.Size([166, 8, 8, 7]) torch.Size([166, 8, 7, 64])\n",
            "torch.Size([150, 8, 10, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 10, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 10, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 10, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 10, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 10, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 9, 9]) torch.Size([150, 8, 9, 64])\n",
            "torch.Size([150, 8, 9, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 9, 9]) torch.Size([150, 8, 9, 64])\n",
            "torch.Size([150, 8, 9, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 9, 9]) torch.Size([150, 8, 9, 64])\n",
            "torch.Size([150, 8, 9, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 9, 9]) torch.Size([150, 8, 9, 64])\n",
            "torch.Size([150, 8, 9, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 9, 9]) torch.Size([150, 8, 9, 64])\n",
            "torch.Size([150, 8, 9, 10]) torch.Size([150, 8, 10, 64])\n",
            "torch.Size([150, 8, 9, 9]) torch.Size([150, 8, 9, 64])\n",
            "torch.Size([150, 8, 9, 10]) torch.Size([150, 8, 10, 64])\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 183, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 111, in main\n",
            "    train_model(model, opt)\n",
            "  File \"train.py\", line 39, in train_model\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 195, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeuYzMG0Dwpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36fe3661-4049-49ad-9717-fa0966275c65"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download fr"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7MB 846kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: fr-core-news-sm\n",
            "  Building wheel for fr-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fr-core-news-sm: filename=fr_core_news_sm-2.2.5-cp36-none-any.whl size=14727027 sha256=3a4b2457e2d6dce260e09b4f04b8e84462fa3210a866273ffdecdc74e29a739d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p9oors45/wheels/46/1b/e6/29b020e3f9420a24c3f463343afe5136aaaf955dbc9e46dfc5\n",
            "Successfully built fr-core-news-sm\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/fr_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/fr\n",
            "You can now load the model via spacy.load('fr')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g2gXEcrEEfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch import optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "# from typing import NamedTuple, List, Callable, Dict, Tuple, Optional\n",
        "from collections import Counter\n",
        "from random import shuffle\n",
        "import numpy as np\n",
        "import gzip\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9VQ9dNrElmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Params:\n",
        "  hidden_size = 512  \n",
        "  shuffle=True\n",
        "  learning_rate=.001\n",
        "  num_epochs=10\n",
        "  data_path = '/content/drive/My Drive/cnndm.gz'\n",
        "  val_data_path = '/content/drive/My Drive/cnndm.val.gz'\n",
        "  eps=1e-31\n",
        "  batch_size=2\n",
        "  model_path='/content/drive/My Drive/model.pt'\n",
        "  # decoder_weights_path='/content/drive/My Drive/decoder.pt'\n",
        "  losses_path='/content/drive/My Drive/val_losses.pkl'\n",
        "\n",
        "  # Testing\n",
        "  test_data_path: str = '/content/drive/My Drive/cnndm.test.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqqqdFsYggRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence:\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxaXhwTfVxh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_tokenizer(text, lower=False, newline=None):\n",
        "  if lower:\n",
        "    text = text.lower()\n",
        "  if newline is not None:  # replace newline by a token\n",
        "    text = text.replace('\\n', ' ' + newline + ' ')\n",
        "  return text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUzjb-jLiAh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "  PAD = 0\n",
        "  SOS = 1\n",
        "  EOS = 2\n",
        "  UNK = 3\n",
        "  def __init__(self):\n",
        "    self.word2index = {}\n",
        "    self.word2count = Counter()\n",
        "    self.reserved = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "    self.index2word = self.reserved[:]\n",
        "    self.embeddings = None\n",
        "\n",
        "  def add_words(self, words):\n",
        "    for word in words:\n",
        "      if word not in self.word2index:\n",
        "        self.word2index[word] = len(self.index2word)\n",
        "        self.index2word.append(word)\n",
        "    self.word2count.update(words)\n",
        "    \n",
        "  def __getitem__(self, item):\n",
        "    if type(item) is int:\n",
        "      return self.index2word[item]\n",
        "    return self.word2index.get(item, self.UNK)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.index2word)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9iq8kTYemMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(nn.Module):\n",
        "  def __init__(self, filename, tokenize=simple_tokenizer):\n",
        "    print(\"Reading dataset %s...\" % filename, end=' ', flush=True)\n",
        "    self.filename = filename\n",
        "    self.pairs = []\n",
        "    self.src_len = 0\n",
        "    self.tgt_len = 0\n",
        "    if filename.endswith('.gz'):\n",
        "      open = gzip.open\n",
        "    with open(filename, 'rt', encoding='utf-8') as f:\n",
        "      for i, line in enumerate(f):\n",
        "        pair = line.strip().split('\\t')\n",
        "        if len(pair) != 2:\n",
        "          print(\"Line %d of %s is malformed.\" % (i, filename))\n",
        "          continue\n",
        "        src = tokenize(pair[0])\n",
        "        tgt = tokenize(pair[1])\n",
        "        self.pairs.append((src, tgt))\n",
        "    print(\"%d pairs.\" % len(self.pairs))\n",
        "\n",
        "  def build_vocab(self):\n",
        "    # word frequency\n",
        "    word_counts={}\n",
        "    count_words(word_counts,[src+tgr for src,tgr in self.pairs])\n",
        "    vocab=Vocab()\n",
        "    for word,count in word_counts.items():\n",
        "        if(count>20):\n",
        "            vocab.add_words([word])  \n",
        "    self.vocab=vocab \n",
        "  def vectorize(self,tokens):\n",
        "    return [self.vocab[token] for token in tokens]\n",
        "  def unvectorize(self, indices):\n",
        "    return [self.vocab[i] for i in indices]\n",
        "  def __getitem__(self, index):\n",
        "    return {'x':self.vectorize(self.pairs[index][0]),\n",
        "            'y':self.vectorize(self.pairs[index][1]),\n",
        "            'src':self.pairs[index][0],\n",
        "            'trg':self.pairs[index][1],\n",
        "            'x_len':len(self.pairs[index][0]),\n",
        "            'y_len':len(self.pairs[index][1])}\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vAylUd4itQL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len = 3000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze_(0)\n",
        "        self.register_buffer('pe', pe)\n",
        " \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # make embeddings relatively larger\n",
        "        x = x * math.sqrt(self.d_model)\n",
        "        #add constant to embedding\n",
        "        seq_len = x.size(1)\n",
        "        pe = torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        x += pe\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVcTFbqpk_OO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(q, k, v, d_k,mask=False):\n",
        "  scores = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(d_k)\n",
        "  scores = F.softmax(scores, dim=-1)  \n",
        "  if mask:\n",
        "    for row in range(scores.size(-2)):\n",
        "      for col in range(scores.size(-1)):\n",
        "        if(row>col):\n",
        "          scores[:,:,row,col]=0  \n",
        "    scores=scores.transpose(-2,-1)           \n",
        "  output = torch.matmul(scores, v)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEn3lW-AkH4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,hidden_size,heads=8):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.heads=heads \n",
        "        self.d_k = hidden_size // heads\n",
        "        self.hidden_size=hidden_size\n",
        "        self.q_linear = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "        self.v_linear = nn.Linear(hidden_size, hidden_size,bias=False)\n",
        "        self.k_linear = nn.Linear(hidden_size, hidden_size,bias=False)        \n",
        "    \n",
        "    def forward(self, q, k, v,mask=False):        \n",
        "        bs = q.size(0)\n",
        "        k = self.k_linear(k).view(bs,-1,self.heads,self.d_k)\n",
        "        q = self.q_linear(q).view(bs,-1,self.heads,self.d_k)\n",
        "        v = self.v_linear(v).view(bs,-1,self.heads,self.d_k)\n",
        "\n",
        "        k=k.transpose(1,2)\n",
        "        q=q.transpose(1,2)\n",
        "        v=v.transpose(1,2)\n",
        "\n",
        "        scores = attention(q, k, v, self.d_k,mask)\n",
        "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.hidden_size)\n",
        "        return concat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZLzjW511Jcf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Norm(nn.Module):\n",
        "  def __init__(self,hidden_size):\n",
        "    super(Norm, self).__init__()\n",
        "    self.norm=nn.LayerNorm(hidden_size,elementwise_affine=False)\n",
        "  def forward(self,x,res):\n",
        "    x+=self.norm(res)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LGMunOsPaRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,hidden_size,d_ff=2048):\n",
        "    super(FeedForward, self).__init__()\n",
        "    self.linear_1=nn.Linear(hidden_size,d_ff)\n",
        "    self.linear_2=nn.Linear(d_ff,hidden_size)\n",
        "  def forward(self,x):\n",
        "    output=self.linear_1(x)\n",
        "    output=self.linear_2(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM6Cs3BLv3ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,hidden_size,vocab_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "    self.pos=PositionalEncoder(hidden_size)\n",
        "    self.embedding=nn.Embedding(vocab_size,hidden_size,padding_idx=0)\n",
        "    self.attn=MultiHeadAttention(hidden_size)\n",
        "    self.norm=Norm(hidden_size)\n",
        "    self.ff=FeedForward(hidden_size)\n",
        "  def forward(self,seq):\n",
        "    embedded=self.embedding(seq)#+self.pos(seq)\n",
        "    embedded+=self.pos(embedded)\n",
        "    output=self.attn(embedded,embedded,embedded)\n",
        "    output=self.norm(embedded,output)\n",
        "    res=self.ff(output)\n",
        "    output=self.norm(output,res)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bVjOsyRR87P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,hidden_size,vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_size=hidden_size\n",
        "    self.pos=PositionalEncoder(hidden_size)\n",
        "    self.embedding=nn.Embedding(vocab_size,hidden_size,padding_idx=0)\n",
        "    self.attn_1=MultiHeadAttention(hidden_size)\n",
        "    self.attn_2=MultiHeadAttention(hidden_size)\n",
        "    self.norm=Norm(hidden_size)\n",
        "    self.ff=FeedForward(hidden_size)\n",
        "  def forward(self,seq,enc_output):\n",
        "    embedded=self.embedding(seq)\n",
        "    embedded+=self.pos(embedded)\n",
        "    output=self.attn_1(embedded,embedded,embedded,True)\n",
        "    output=self.norm(embedded,output)\n",
        "    res=self.attn_2(output,enc_output,enc_output)\n",
        "    output=self.norm(output,res)\n",
        "    res=self.ff(output)\n",
        "    dec_output=self.norm(output,res)\n",
        "    return output,dec_output,embedded     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsHI4LD01Uut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,vocab,hidden_size):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.vocab=vocab\n",
        "    self.vocab_size=len(vocab)\n",
        "    self.hidden_size=hidden_size\n",
        "    self.encoder=Encoder(hidden_size,self.vocab_size)\n",
        "    self.decoder=Decoder(hidden_size,self.vocab_size)\n",
        "    self.linear=nn.Linear(hidden_size,self.vocab_size)\n",
        "    self.softmax=nn.Softmax()\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.attn=MultiHeadAttention(hidden_size) \n",
        "    # self.enc_dim=nn.Parameter(torch.tensor(1),requires_grad=False)\n",
        "    # self.dec_dim=nn.Parameter(torch.tensor(1),requires_grad=False)\n",
        "    # self.embd_dim=nn.Parameter(torch.tensor(1),requires_grad=False)\n",
        "    self.enc_linear=nn.Linear(hidden_size,1)\n",
        "    self.dec_linear=nn.Linear(hidden_size,1)\n",
        "    self.embd_linear=nn.Linear(hidden_size,1)\n",
        "\n",
        "  def forward(self,x,y,extra_vocab,expanded_word_idx):\n",
        "    enc_output=self.encoder(x)\n",
        "    output,dec_output,embedded=self.decoder(y,enc_output)\n",
        "    gen_output=self.softmax(self.linear(dec_output))\n",
        "    ptr_output=self.attn(output,enc_output,enc_output)    \n",
        "    \n",
        "    ptr_mean=torch.mean(ptr_output,-2).squeeze_()\n",
        "    dec_output_mean=torch.mean(dec_output,-2).squeeze_()\n",
        "    embedded_mean=torch.mean(embedded,-2).squeeze_()\n",
        "    p_gen=self.sigmoid(self.enc_linear(ptr_mean)+self.dec_linear(dec_output_mean)+self.embd_linear(embedded_mean))\n",
        "    gen_output=F.pad(gen_output,(0,len(extra_vocab)),'constant')\n",
        "        \n",
        "    ptr_dist=torch.matmul(output, enc_output.transpose(-2,-1))    \n",
        "    ptr_dist=F.softmax(ptr_dist, dim=-1)\n",
        "    gen_output.scatter_add_(2,expanded_word_idx.unsqueeze_(1).expand(-1,gen_output.size(1),-1),ptr_dist*(1-p_gen).unsqueeze_(2))\n",
        "    return torch.log(gen_output+1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4PVqcOrqZDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def my_collate(batch):\n",
        "    max_x=np.max([item['x_len'] for item in batch])\n",
        "    max_y=np.max([item['y_len'] for item in batch])\n",
        "    \n",
        "    x = [F.pad(torch.tensor(item['x']), (0,max_x-len(item['x'])), 'constant').tolist() for item in batch]    \n",
        "    y = [F.pad(torch.tensor(item['y']), (0,max_y-len(item['y'])), 'constant').tolist() for item in batch]\n",
        "    \n",
        "    src=[item['src'] for item in batch]\n",
        "    trg=[item['trg'] for item in batch]\n",
        "    return {'x':x,'y': y,'src':src,'trg':trg,'x_len':[item['x_len'] for item in batch],'y_len':[item['y_len'] for item in batch]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiZTtodt1Xfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "791a16c3-db22-4251-8bf8-751085b2ce07"
      },
      "source": [
        "with torch.autograd.set_detect_anomaly(True):\n",
        "  p=Params()\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  batch_size=p.batch_size\n",
        "  dataset=Dataset(p.data_path)\n",
        "  dataset.build_vocab()\n",
        "  dataloader = DataLoader(dataset,p.batch_size,p.shuffle,collate_fn=my_collate,pin_memory=True)\n",
        "  model=Transformer(dataset.vocab,p.hidden_size).to(device)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=p.learning_rate)\n",
        "  criterion = nn.NLLLoss(ignore_index=dataset.vocab.PAD)\n",
        "\n",
        "  for _e in range(p.num_epochs):\n",
        "    for batch in tqdm(dataloader):\n",
        "      extra_vocab={}\n",
        "      expanded_x=[]\n",
        "      expanded_y=[]\n",
        "      \n",
        "      x=torch.tensor(batch['x']).to(device)\n",
        "      y=torch.tensor(batch['y']).to(device)\n",
        "      src=batch['src']\n",
        "      trg=batch['trg']\n",
        "\n",
        "      max_src=np.max(batch['x_len'])\n",
        "      for sent in src:\n",
        "        vec=[]\n",
        "        for token in sent:\n",
        "          if(token in extra_vocab.keys()):\n",
        "            vec.append(extra_vocab[token]) \n",
        "          elif(token not in dataset.vocab.word2index):\n",
        "            extra_vocab[token]=len(dataset.vocab)+len(extra_vocab)\n",
        "            vec.append(extra_vocab[token])\n",
        "        expanded_x.append(vec + [0 for i in range(max_src-len(vec))])\n",
        "      expanded_x=torch.tensor(expanded_x).to(device)  \n",
        "\n",
        "      max_trg=np.max(batch['y_len'])\n",
        "      for sent in trg:\n",
        "        vec=[]\n",
        "        for token in sent:\n",
        "          if(token in extra_vocab.keys()):\n",
        "            vec.append(extra_vocab[token]) \n",
        "          elif(token not in dataset.vocab.word2index):\n",
        "            extra_vocab[token]=len(dataset.vocab)+len(extra_vocab)\n",
        "            vec.append(extra_vocab[token])\n",
        "        expanded_y.append(vec + [0 for i in range(max_trg-len(vec))])          \n",
        "      expanded_y = torch.tensor(expanded_y).to(device)\n",
        "      \n",
        "      y=torch.cat((torch.tensor([dataset.vocab.SOS] * y.size(0), device=device).view(-1,1),y[:,:-1]),dim=1)\n",
        "      pred=model(x,y,extra_vocab,expanded_x)\n",
        "      loss=0\n",
        "      for i in range(pred.size(1)):\n",
        "        loss+=criterion(pred[:,i,:].squeeze_(),expanded_y[:,i])\n",
        "      # loss=criterion(pred,expanded_y.unsqueeze(2))\n",
        "      loss.backward()\n",
        "      optimizer.step() \n",
        "      print(loss.data.item())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading dataset /content/drive/My Drive/cnndm.gz... 287089 pairs.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/143545 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  0%|          | 0/143545 [00:03<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3c87abc6935b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpanded_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;31m# loss=criterion(pred,expanded_y.unsqueeze(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2, 92915]], which is output 0 of AsStridedBackward, is at version 64; expected version 63 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHk6HO4HOluF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "32d514f2-5b97-443e-c81d-f88a15d5b873"
      },
      "source": [
        "a=torch.randn(2,2)\n",
        "# a += torch.randn(1,2)\n",
        "a."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9398,  1.5127],\n",
              "        [-1.0192,  0.6220]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai3rniCrZiNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}